{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(images, n_patches):\n",
    "    n,c,h,w = images.shape\n",
    "    patches = torch.zeros(n,n_patches**2, h*w//n_patches**2) #(34000, 100,25)\n",
    "    patch_size = h//n_patches\n",
    "\n",
    "    for idx,image in enumerate(images):\n",
    "        for i in range(n_patches):\n",
    "            for j in range(n_patches):\n",
    "                patch = image[:,i*patch_size:(i+1)*patch_size,j*patch_size:(j+1)*patch_size]\n",
    "                patches[idx,i*n_patches+j] = patch.flatten()\n",
    "    return patches\n",
    "\n",
    "def get_positional_embeddings(sequence_length,d):\n",
    "    result = torch.ones(sequence_length,d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i/(10000**(j/d))) if j%2==0 else np.cos(i/(10000**((j-1)/d)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSA(nn.Module):\n",
    "    def __init__(self, d, n_heads=1):\n",
    "        super(MSA, self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        d_head = int(d / n_heads)\n",
    "        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
    "\n",
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "        super(ViTBlock, self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MSA(hidden_d, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mhsa(self.norm1(x))\n",
    "        out = out + self.mlp(self.norm2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tang_vit(nn.Module):\n",
    "    def __init__(self,chw = (1,50,50),n_patches =10,n_blocks=1,hidden_d=200, n_heads=1):\n",
    "        super(tang_vit,self).__init__()\n",
    "\n",
    "        self.chw = chw\n",
    "        self.n_patches = n_patches\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_d = hidden_d\n",
    "\n",
    "        self.patch_size = (chw[1]/n_patches,chw[2]/n_patches)\n",
    "\n",
    "        \n",
    "        #linear mapping of patches (could also convolve)\n",
    "        self.input_d = int(chw[0]*self.patch_size[0]*self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input_d,self.hidden_d) #runnig 1:1 map from (3400,100,25) through a (25,12) mapper. so only happens on last dim\n",
    "\n",
    "        #class token (learnable)\n",
    "        #self.class_token = nn.Parameter(torch.rand(1,self.hidden_d))\n",
    "\n",
    "        #pos embed\n",
    "        self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(self.n_patches**2,self.hidden_d)))\n",
    "        self.pos_embed.requires_grad=False\n",
    "\n",
    "        #transformer blocks\n",
    "        self.blocks = nn.ModuleList([ViTBlock(hidden_d,n_heads) for _ in range(n_blocks)])\n",
    "\n",
    "        #flatten\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #linear\n",
    "        self.linear = nn.Linear(100*200,1) #nn.Linear(self.hidden_d,1)\n",
    "\n",
    "    def forward(self, images):\n",
    "        n,c,h,w = images.shape\n",
    "        patches = patchify(images, self.n_patches)\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        #We can now add a parameter to our model and convert our (N, 100, 12) tokens tensor to an (N, 101, 12) tensor (we add the special token to each sequence).\n",
    "        #added at front\n",
    "        #tokens = torch.stack([torch.vstack((self.class_token,tokens[i])) for i in range(len(tokens))])\n",
    "\n",
    "        #add pos embed\n",
    "        pos_embed = self.pos_embed.repeat(n,1,1) #tokens have size (3400, 101, 12)\n",
    "        out = tokens+pos_embed\n",
    "\n",
    "        #transformer\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "        \n",
    "        out = self.flatten(out)\n",
    "        return self.linear(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34000, 1, 50, 50)\n",
      "(34000, 302)\n"
     ]
    }
   ],
   "source": [
    "img = np.load('../cnn-neuron/all_sites_data_prepared/pics_data/train_img_m1s1.npy')\n",
    "resp = np.load('../cnn-neuron/all_sites_data_prepared/New_response_data/trainRsp_m1s1.npy')\n",
    "img=np.reshape(img,(34000,1,50,50))\n",
    "print(img.shape)\n",
    "print(resp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y0/p78gsyrd5wv23ft_8q993_9r0000gn/T/ipykernel_17105/3899434159.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(self.n_patches**2,self.hidden_d)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3400, 1])\n"
     ]
    }
   ],
   "source": [
    "net = tang_vit()\n",
    "x=torch.randn(3400,1,50,50)\n",
    "print(net(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, labels, num_neurons):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.num_neurons = num_neurons\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = torch.tensor(self.data[index], dtype=torch.float)\n",
    "        label = torch.tensor(self.labels[index, 0:self.num_neurons], dtype=torch.float)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y0/p78gsyrd5wv23ft_8q993_9r0000gn/T/ipykernel_17105/3899434159.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(self.n_patches**2,self.hidden_d)))\n"
     ]
    }
   ],
   "source": [
    "numneurons = 1\n",
    "dataset = ImageDataset(img,resp,numneurons)\n",
    "loader = DataLoader(dataset,8,shuffle=True)\n",
    "net = tang_vit()#models 1 neuron\n",
    "opt = torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "lfunc = torch.nn.MSELoss()\n",
    "vimg = np.load('../cnn-neuron/all_sites_data_prepared/pics_data/val_img_m1s1.npy')\n",
    "vresp = np.load('../cnn-neuron/all_sites_data_prepared/New_response_data/valRsp_m1s1.npy')\n",
    "vimg = np.reshape(vimg,(1000,1,50,50))\n",
    "vdataset = ImageDataset(vimg,vresp,numneurons)\n",
    "vloader = DataLoader(vdataset,8,shuffle=True)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: saved sucessfully.\n",
      "Epoch 0 train loss is:0.024757306880153276\n",
      "Epoch 0 val loss is:0.0032671628722455355\n",
      "Epoch 1: saved sucessfully.\n",
      "Epoch 1 train loss is:8.00258180540744\n",
      "Epoch 1 val loss is:0.06536336320638657\n",
      "Epoch 2: saved sucessfully.\n",
      "Epoch 2 train loss is:0.1341056477539789\n",
      "Epoch 2 val loss is:0.010496995819732546\n",
      "Epoch 3: saved sucessfully.\n",
      "Epoch 3 train loss is:0.0537286033630645\n",
      "Epoch 3 val loss is:0.01022425043862313\n",
      "Epoch 4: saved sucessfully.\n",
      "Epoch 4 train loss is:15.464959331746021\n",
      "Epoch 4 val loss is:0.6926353993415832\n",
      "Epoch 5: saved sucessfully.\n",
      "Epoch 5 train loss is:0.7559719703061067\n",
      "Epoch 5 val loss is:0.03489301681518555\n",
      "Epoch 6: saved sucessfully.\n",
      "Epoch 6 train loss is:0.12313412522562944\n",
      "Epoch 6 val loss is:0.04645688395947218\n",
      "Epoch 7: saved sucessfully.\n",
      "Epoch 7 train loss is:1.5175906933365555\n",
      "Epoch 7 val loss is:0.025122982339933514\n",
      "Epoch 8: saved sucessfully.\n",
      "Epoch 8 train loss is:0.10253599020299119\n",
      "Epoch 8 val loss is:0.010512489390093834\n",
      "Epoch 9: saved sucessfully.\n",
      "Epoch 9 train loss is:0.07477349195007564\n",
      "Epoch 9 val loss is:0.017793391302227974\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "for epoch in range(10):\n",
    "    net.train()\n",
    "    train_losses=[]\n",
    "    for x,y in loader:\n",
    "        l = lfunc(net(x),y)\n",
    "        opt.zero_grad()\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "        train_losses.append(l.item())\n",
    "    losses.append(np.mean(train_losses))\n",
    "    \n",
    "    print(\"Epoch \" + str(epoch) + \": saved sucessfully.\")\n",
    "    torch.save(net.state_dict(), './model13.pth')\n",
    "    \n",
    "    val_losses=[]\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for x,y in vloader:\n",
    "            l = lfunc(net(x),y)\n",
    "            val_losses.append(l.item())\n",
    "    accs.append(np.mean(val_losses))\n",
    "\n",
    "    print(\"Epoch \" + str(epoch) + \" train loss is:\" + str(losses[-1]))\n",
    "    print(\"Epoch \" + str(epoch) + \" val loss is:\" + str(accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.asarray([item[0:1] for item in vresp])\n",
    "responses = torch.from_numpy(np.float32(vimg))\n",
    "with torch.no_grad():\n",
    "        net.eval()\n",
    "        responses2 = net(responses).detach().numpy()\n",
    "images = np.transpose(images)\n",
    "responses2 = np.transpose(responses2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11776056838316692]\n",
      "0.11776056838316692\n"
     ]
    }
   ],
   "source": [
    "corrs = []\n",
    "for i in range(1):\n",
    "    corrs.append(np.corrcoef(images[i], responses2[i])[0][1])\n",
    "print(corrs)\n",
    "print(np.mean(corrs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leelab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de6698c26580af88e532a4b7983a0dee6e4f27991771fe8c6c6d14abebdd338c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
