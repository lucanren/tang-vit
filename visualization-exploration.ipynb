{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "def patchify(images, n_patches):\n",
    "    n,c,h,w = images.shape\n",
    "    patches = torch.zeros(n,n_patches**2, h*w//n_patches**2) #(34000, 100,25)\n",
    "    patch_size = h//n_patches\n",
    "\n",
    "    for idx,image in enumerate(images):\n",
    "        for i in range(n_patches):\n",
    "            for j in range(n_patches):\n",
    "                patch = image[:,i*patch_size:(i+1)*patch_size,j*patch_size:(j+1)*patch_size]\n",
    "                patches[idx,i*n_patches+j] = patch.flatten()\n",
    "    return patches\n",
    "\n",
    "def get_positional_embeddings(sequence_length,d):\n",
    "    result = torch.ones(sequence_length,d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i/(10000**(j/d))) if j%2==0 else np.cos(i/(10000**((j-1)/d)))\n",
    "    return result\n",
    "\n",
    "class MSA(nn.Module):\n",
    "    def __init__(self, d, n_heads=2):\n",
    "        super(MSA, self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        d_head = int(d / n_heads)\n",
    "        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
    "\n",
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "        super(ViTBlock, self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MSA(hidden_d, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mhsa(self.norm1(x))\n",
    "        out = out + self.mlp(self.norm2(out))\n",
    "        return out\n",
    "\n",
    "class tang_vit(nn.Module):\n",
    "    def __init__(self,chw = (1,50,50),n_patches =10,n_blocks=4,hidden_d=12, n_heads=2):\n",
    "        super(tang_vit,self).__init__()\n",
    "\n",
    "        self.chw = chw\n",
    "        self.n_patches = n_patches\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_d = hidden_d\n",
    "\n",
    "        self.patch_size = (chw[1]/n_patches,chw[2]/n_patches)\n",
    "\n",
    "        \n",
    "        #linear mapping of patches (could also convolve)\n",
    "        self.input_d = int(chw[0]*self.patch_size[0]*self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input_d,self.hidden_d) #runnig 1:1 map from (3400,100,25) through a (25,12) mapper. so only happens on last dim\n",
    "\n",
    "        #class token (learnable)\n",
    "        #self.class_token = nn.Parameter(torch.rand(1,self.hidden_d))\n",
    "\n",
    "        #pos embed\n",
    "        self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(self.n_patches**2,self.hidden_d)))\n",
    "        self.pos_embed.requires_grad=False\n",
    "\n",
    "        #transformer blocks\n",
    "        self.blocks = nn.ModuleList([ViTBlock(hidden_d,n_heads) for _ in range(n_blocks)])\n",
    "\n",
    "        #flatten\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        #linear\n",
    "        self.linear = nn.Linear((n_patches**2) *hidden_d,1) #nn.Linear(self.hidden_d,1)\n",
    "\n",
    "    def forward(self, images):\n",
    "        n,c,h,w = images.shape\n",
    "        patches = patchify(images, self.n_patches)\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        #We can now add a parameter to our model and convert our (N, 100, 12) tokens tensor to an (N, 101, 12) tensor (we add the special token to each sequence).\n",
    "        #added at front\n",
    "        #tokens = torch.stack([torch.vstack((self.class_token,tokens[i])) for i in range(len(tokens))])\n",
    "\n",
    "        #add pos embed\n",
    "        pos_embed = self.pos_embed.repeat(n,1,1) #tokens have size (3400, 101, 12)\n",
    "        out = tokens+pos_embed\n",
    "\n",
    "        #transformer\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "        \n",
    "        out = self.flatten(out)\n",
    "        return self.linear(out)\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, labels, num_neurons):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.num_neurons = num_neurons\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = torch.tensor(self.data[index], dtype=torch.float)\n",
    "        label = torch.tensor(self.labels[index, 0:self.num_neurons], dtype=torch.float)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34000, 1, 50, 50)\n",
      "(34000, 302)\n"
     ]
    }
   ],
   "source": [
    "img = np.load('../all_sites_data_prepared/pics_data/train_img_m1s1.npy')\n",
    "resp = np.load('../all_sites_data_prepared/New_response_data/trainRsp_m1s1.npy')\n",
    "img=np.reshape(img,(34000,1,50,50))\n",
    "print(img.shape)\n",
    "print(resp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y0/p78gsyrd5wv23ft_8q993_9r0000gn/T/ipykernel_30469/2789985973.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(self.n_patches**2,self.hidden_d)))\n"
     ]
    }
   ],
   "source": [
    "numneurons = 1\n",
    "dataset = ImageDataset(img,resp,numneurons)\n",
    "loader = DataLoader(dataset,8,shuffle=True)\n",
    "net = tang_vit(n_patches=5,n_blocks=4,hidden_d=12,n_heads=2)#models 1 neuron\n",
    "opt = torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "lfunc = torch.nn.MSELoss()\n",
    "vimg = np.load('../all_sites_data_prepared/pics_data/val_img_m1s1.npy')\n",
    "vresp = np.load('../all_sites_data_prepared/New_response_data/valRsp_m1s1.npy')\n",
    "vimg = np.reshape(vimg,(1000,1,50,50))\n",
    "vdataset = ImageDataset(vimg,vresp,numneurons)\n",
    "vloader = DataLoader(vdataset,len(vresp),shuffle=False)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"./model-best5.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3056501515331857]\n",
      "0.3056501515331857\n"
     ]
    }
   ],
   "source": [
    "images = np.asarray([item[0:1] for item in vresp])\n",
    "responses = torch.from_numpy(np.float32(vimg))\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    responses2 = net(responses).detach().numpy()\n",
    "images = np.transpose(images)\n",
    "responses2 = np.transpose(responses2)\n",
    "corrs = []\n",
    "for i in range(1):\n",
    "    corrs.append(np.corrcoef(images[i], responses2[i])[0][1])\n",
    "    print(corrs)\n",
    "print(np.mean(corrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leelab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de6698c26580af88e532a4b7983a0dee6e4f27991771fe8c6c6d14abebdd338c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
